{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home1/shubhamg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "###### Pre-Processing Function \n",
    "import re\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def pre_process(name,s=False,l=False):\n",
    "    #1. replace , ;  with a whitespace\n",
    "    \n",
    "    #2. remove . \" ' ( ) \n",
    "    name = re.sub(\"[\\\"\\'.()\\[\\]\\{\\}<>`\\?\\!#\\$%^\\&=@]\", \" \", name)\n",
    "    name = re.sub('https?://[A-Za-z0-9./]+','',name)\n",
    "    \n",
    "    \n",
    "    name = re.sub(\"[,;:\\+/\\\\\\*~\\|]\", \" \", name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3. replace multiple spaces with single space\n",
    "    name = \" \".join(name.strip().split())\n",
    "    \n",
    "    #4. convert string to lower case\n",
    "    name = name.lower()\n",
    "    \n",
    "    \n",
    "    #5. Removal of stop words\n",
    "    name =  \" \".join(x for x in name.split() if x not in stop)\n",
    "    \n",
    "    #6. Stemming\n",
    "    if (s):\n",
    "        name = \" \".join([st.stem(name) for name in x.split()])\n",
    "        \n",
    "    \n",
    "    #7. Lemmatization\n",
    "    if (l):\n",
    "        name = \" \".join([Word(word).lemmatize() for word in name.split()])\n",
    "        \n",
    "        \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TF_IDF(data,name,char=False):\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=100000,\n",
    "        use_idf=False)\n",
    "    word_vectorizer.fit(data[name])\n",
    "    \n",
    "    features_word = word_vectorizer.transform(data[name])\n",
    "    a = list(word_vectorizer.vocabulary_.keys())\n",
    "    word_vol = [\"W_\" + s for s in a]\n",
    "\n",
    "    if (char):\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='char',\n",
    "            stop_words='english',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=50000,\n",
    "            use_idf=False)\n",
    "        char_vectorizer.fit(data[name])\n",
    "        features_char = char_vectorizer.transform(data[name])\n",
    "        \n",
    "        features = hstack([features_word, features_char])\n",
    "        \n",
    "        b = list(char_vectorizer.vocabulary_.keys())\n",
    "        char_vol = [\"C_\" + s for s in b]\n",
    "        feature_cols = word_vol + char_vol\n",
    "    else: \n",
    "        features = features_word\n",
    "        feature_cols = word_vol\n",
    "        \n",
    "    return features,word_vectorizer, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emoticons_Positive(name):\n",
    "    pos_count = 0\n",
    "    pos_emoticons = [\":-)\",\":)\",\"(:\",\"(-:\",\"<3\",\":*\",\":-D\",\":D\",\"X-D\",\"XD\",\"xD\",\n",
    "                      \";-)\",\";)\",\";-D\",\";D\",\"(;\",\"(-;\"]\n",
    "    for emoticons in pos_emoticons:\n",
    "        pos_count+=name.count(emoticons)\n",
    "    return pos_count\n",
    "\n",
    "def Emoticons_Negative(name):\n",
    "    neg_count = 0\n",
    "    neg_emoticons = [\":-(\",\":(\",\n",
    "                      \":,(\",\":'(\",\":((\"]\n",
    "    for emoticons in neg_emoticons:\n",
    "        neg_count+=name.count(emoticons)\n",
    "    return neg_count\n",
    "\n",
    "def negative_comments(name):\n",
    "    neg_count = 0\n",
    "    name = name.lower()\n",
    "    neg_comments = [\"no\", \"not\", \"doesn't\", \"does not\", \"don't\"]\n",
    "    for comments in neg_comments:\n",
    "        neg_count+=name.count(comments)\n",
    "    return neg_count\n",
    "\n",
    "\n",
    "def Punc_help(name):\n",
    "    help_count = 0\n",
    "    #punc_help = [\"...\"]\n",
    "    help_count=name.count(\"...\")\n",
    "    help_count=help_count - name.count(\"....\")\n",
    "        \n",
    "\n",
    "    return help_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "train_full = pd.read_csv('/data1/shubham.gupta/competitions/Linguipedia/data/train.csv')\n",
    "test = pd.read_csv('/data1/shubham.gupta/competitions/Linguipedia/data/test.csv')\n",
    "\n",
    "#train = train_full.iloc[:6000]\n",
    "#test = train_full.iloc[6000:]\n",
    "\n",
    "train['tweet_pre'] = train['tweet'].apply(pre_process,l=True)\n",
    "test['tweet_pre'] = test['tweet'].apply(pre_process,l=True)\n",
    "frames = [train[['id','tweet_pre']], test[['id','tweet_pre']]]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "features,word_vectorizer,feature_cols = TF_IDF(df,'tweet_pre',char=True)\n",
    "#features,word_vectorizer,feature_cols = TF_IDF(df,'tweet_pre')\n",
    "features = pd.DataFrame(features.todense())\n",
    "features.columns = feature_cols\n",
    "\n",
    "frames_1 = pd.concat([train[['id','tweet','tweet_pre']], test[['id','tweet','tweet_pre']]])\n",
    "frames_1.index = range(0,len(frames_1))\n",
    "special_char = ['&',\"\\$\",\"@\",\"#\",\"\\*\"]\n",
    "features[\"\\$&@\\*#_Cnt\"] = frames_1.tweet.str.count(\"\\$&@\\*#\")\n",
    "for spe_chr in special_char:\n",
    "    col_name = spe_chr + \"_Cnt\"\n",
    "    features[col_name] = frames_1.tweet.str.count(spe_chr)\n",
    "    features[col_name] = features[col_name] - features[\"\\$&@\\*#_Cnt\"]\n",
    "    #features[col_name] = np.where(features[col_name]>0,1,0)\n",
    "\n",
    "features[\"insta_Cnt\"] = frames_1.tweet.str.count(\"instagram\")\n",
    "features[\"http_Cnt\"] = frames_1.tweet.str.count(\"http\")\n",
    "features['Post_Cnt'] = frames_1['tweet'].apply(Emoticons_Positive)\n",
    "features['Neg_Cnt'] = frames_1['tweet'].apply(Emoticons_Negative)\n",
    "features['Neg_Comment_Cnt'] = frames_1['tweet'].apply(negative_comments)\n",
    "features['Help_Cnt'] = frames_1['tweet'].apply(Punc_help)\n",
    "features['word_Cnt'] = frames_1['tweet_pre'].str.count(\" \")\n",
    "#features[\"insta&***_Cnt\"] = (features['insta_Cnt']+ features['http_Cnt'])*features['\\$&@\\*#_Cnt']\n",
    "#features[\"insta&***_Cnt\"] = np.where(features[\"insta&***_Cnt\"]>0,1,0)\n",
    "\n",
    "\n",
    "\n",
    "train_features = features.iloc[:len(train)]\n",
    "test_features = features.iloc[len(train):]\n",
    "target = train.label\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, target)\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "test['label'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430962059274548"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(target, model.predict(train_features), average='weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28776241679467485, 0.2558080808080808)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.mean(), target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','label']].to_csv('/data1/shubham.gupta/competitions/Linguipedia/submission/06_LR_Emot_BiGram_Abuse_Insta_Neg_Char_SC_http_l1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
